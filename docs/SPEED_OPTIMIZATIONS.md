# ‚ö° Optimizaciones de Velocidad - Clinical Assistant

**Fecha**: 5 de Diciembre, 2025  
**Comparaci√≥n**: mental_health_api-main ‚Üí clinical_assistant

---

## üî• CAMBIOS CR√çTICOS IMPLEMENTADOS

### 1. **Singleton Pattern con Lazy Loading** (IMPACTO ALTO)

**Antes:**
```python
# Variables globales simples
classification_model = None

def load_classification_model():
    # ‚ùå Sin check de "ya cargado"
    classification_model = AutoModelForSequenceClassification.from_pretrained(...)
```

**Despu√©s:**
```python
class ModelManager:
    def load_classifier(self):
        if self.cls_model is not None:
            return True  # ‚ö° Ya cargado, salir inmediatamente
        
        self.cls_model = AutoModelForSequenceClassification.from_pretrained(...)
```

**Resultado**: Evita recargas innecesarias de modelos entre peticiones.

---

### 2. **Pipeline Consolidado** (IMPACTO ALTO)

**Antes:**
- Funciones separadas: `classify_mental_health()`, `generate_summary()`, `generate_recommendation()`
- Overhead de llamadas entre funciones
- Movimiento manual de tensores a dispositivos

**Despu√©s:**
```python
def process_request(self, text, auto_classify=True, pathology=None):
    # TODO en una sola funci√≥n
    # 1. Clasificaci√≥n ‚Üí 2. Summarization ‚Üí 3. Generation
    # Sin overhead de funciones separadas
    return result
```

**Resultado**: Menos overhead, procesamiento m√°s directo.

---

### 3. **T5 Pipeline Optimizado** (IMPACTO MEDIO)

**Antes:**
```python
# Modelo crudo (para compatibilidad MPS)
t5_model = AutoModelForSeq2SeqLM.from_pretrained(...)
t5_model = t5_model.to("mps")
t5_summarizer = {"model": t5_model, "tokenizer": t5_tokenizer}
```

**Despu√©s:**
```python
if device == "cuda":
    # ‚ö° Pipeline optimizado para CUDA
    self.sum_pipeline = pipeline("summarization", model=model, device=0)
else:
    # Fallback para MPS/CPU
    self.sum_model = model.to(device)
```

**Resultado**: CUDA usa pipeline optimizado, MPS/CPU usan modelo crudo.

---

### 4. **Device Handling Din√°mico** (IMPACTO MEDIO)

**Antes:**
```python
# Hard-coded en config
DEVICE = "cuda" if torch.cuda.is_available() else "mps" if ...
```

**Despu√©s:**
```python
def get_device():
    """Detecta autom√°ticamente el mejor dispositivo"""
    if torch.cuda.is_available():
        return "cuda"
    elif torch.backends.mps.is_available():
        return "mps"
    return "cpu"
```

**Resultado**: Configuraci√≥n autom√°tica sin hard-coding.

---

### 5. **Endpoint de Status** (IMPACTO UI/DEBUG)

**Nuevo endpoint:**
```python
@router.get("/get_status")
def get_status():
    return {"status": "ok", "device": get_device()}
```

**Frontend:**
```javascript
// Muestra dispositivo en UI
async function loadExecutionDevice() {
    const data = await fetch(STATUS_URL).json();
    executionDevice.textContent = getDeviceIcon(data.device);
}
```

**Resultado**: Usuario ve qu√© hardware est√° usando (CUDA/MPS/CPU).

---

## üìä IMPACTO ESPERADO

| M√©trica | Antes | Despu√©s | Mejora |
|---------|-------|---------|--------|
| **Primera petici√≥n** | ~30-60s | ~30-60s | 0% (igual - carga inicial) |
| **Peticiones subsecuentes** | ~25-40s | ~5-10s | **60-80% m√°s r√°pido** |
| **Uso de memoria** | Variable | Estable | Sin recargas innecesarias |
| **Startup** | Secuencial | Precarga | Todos los modelos listos |

---

## üß™ C√ìMO PROBAR

### Opci√≥n 1: Backend local + Frontend

```bash
# Terminal 1: Backend
cd backend
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000

# Terminal 2: Ver logs del backend
# Observa los mensajes de "‚ö° Already loaded, skip"
```

Luego abre `frontend/index.html` en el navegador.

### Opci√≥n 2: Docker

```bash
docker-compose up --build
```

---

## üîç VERIFICACI√ìN DE OPTIMIZACIONES

### Check 1: Lazy Loading
Busca en los logs del backend:

```
Primera petici√≥n:
üîç Loading classification model...
‚úÖ Classification model loaded on MPS

Segunda petici√≥n:
(No aparece "Loading classification model" - usa cach√©)
```

### Check 2: Tiempo de procesamiento
Busca al final de cada petici√≥n:

```
‚è±Ô∏è Total processing time: 7.32 seconds
```

### Check 3: Device Status
En el frontend, deber√≠as ver en el header:

```
Execution Device: üçé GPU (Apple Silicon)
```

---

## üöÄ SIGUIENTES PASOS (OPCIONALES)

Si quieres a√∫n m√°s velocidad:

1. **Quantizaci√≥n INT8**: Reduce precisi√≥n para mayor velocidad
2. **Batch Processing**: Procesa m√∫ltiples casos en paralelo
3. **Response Streaming**: Muestra resultados parciales mientras genera
4. **Model Caching en Disco**: Precarga modelos desde cache PyTorch

---

## üìù NOTAS T√âCNICAS

- **MPS Stability**: Llama se mantiene en CPU para MPS por estabilidad (conocido issue)
- **Pipeline vs Raw**: Pipeline es m√°s r√°pido en CUDA, no disponible en MPS
- **Singleton Global**: `manager = ModelManager()` se crea una vez al importar
- **Backward Compatibility**: Funciones legacy (`load_all_models()`, etc.) redirigen al manager

---

## üêõ TROUBLESHOOTING

**Problema**: "Models not loaded"
- **Soluci√≥n**: Verifica que los modelos est√©n en `backend/models/`

**Problema**: Frontend no muestra dispositivo
- **Soluci√≥n**: Verifica que backend est√© corriendo y accesible en `localhost:8000`

**Problema**: Sigue siendo lento
- **Soluci√≥n**: 
  1. Verifica logs - ¬øaparece "Loading model" en cada petici√≥n?
  2. Comprueba que `manager.cls_model is not None` despu√©s de la primera carga
  3. Revisa que no haya errores de importaci√≥n

---

**Autor**: GitHub Copilot  
**Basado en**: An√°lisis comparativo con mental_health_api-main
