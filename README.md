# ğŸ§  Clinical Mental Health Assistant

> **AI-Powered Diagnostic Support Tool for Mental Health Professionals**

An intelligent system that provides automated mental health condition classification and evidence-based treatment recommendations based on clinical case descriptions.

[![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://www.python.org/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104-green.svg)](https://fastapi.tiangolo.com/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.1-red.svg)](https://pytorch.org/)

---

## ğŸ“‹ Table of Contents

- [Overview](#-overview)
- [Features](#-features)
- [Architecture](#-architecture)
- [Quick Start](#-quick-start)
  - [Docker Deployment](#-docker-deployment-recommended)
  - [Local Development](#-local-development)
- [Project Structure](#-project-structure)
- [API Documentation](#-api-documentation)
- [Configuration](#-configuration)
- [Troubleshooting](#-troubleshooting)
- [Documentation](#-documentation)
- [Disclaimer](#ï¸-important-disclaimer)

---

## ğŸ¯ Overview

The Clinical Mental Health Assistant is a **unified full-stack application** that leverages three state-of-the-art NLP models:

| Component | Model | Purpose | Performance |
|-----------|-------|---------|-------------|
| **Classification** | BERT (110M params) | Diagnose mental health conditions | 204K training samples |
| **Summarization** | T5-base (220M params) | Extract clinical summaries | ROUGE-2: 14.72% |
| **Generation** | Llama 3.2-1B + LoRA | Generate treatment recommendations | Perplexity: 6.15 |

### âœ¨ Key Features

- âœ… **Unified Deployment**: Single command launch - no separate frontend/backend
- âš¡ **High Performance**: 60-80% faster inference with optimized model loading
- ğŸ”’ **Privacy-First**: All processing runs locally, no external API calls
- ğŸ¯ **Professional UI**: Clean, medical-grade interface
- ğŸ“Š **Real-time Status**: Device monitoring (CUDA/MPS/CPU)
- ğŸ³ **Docker Ready**: Production-ready containerized deployment

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              FastAPI Application (Port 8000)                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Frontend (/)              â”‚  Backend API (/api/v1)          â”‚
â”‚  - HTML/CSS/JS             â”‚  - /analyze (POST)              â”‚
â”‚  - Served by FastAPI       â”‚  - /health (GET)                â”‚
â”‚                            â”‚  - /get_status (GET)            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚              Model Manager (Singleton Pattern)               â”‚
â”‚  â€¢ Lazy Loading            â”‚  â€¢ Device Auto-detection        â”‚
â”‚  â€¢ Model Caching           â”‚  â€¢ Unified Pipeline             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Design Principles:**
- **Unified Stack**: Frontend and backend served from single FastAPI instance
- **Singleton Pattern**: Models loaded once and reused across requests
- **Lazy Loading**: Models load only when needed, with caching
- **Zero Config**: Automatic device detection (CUDA/MPS/CPU)

---

## ğŸš€ Quick Start

### ğŸ³ Docker Deployment (Recommended)

**Perfect for production or if you want zero setup hassle.**

```bash
# 1. Clone the repository
git clone https://github.com/gortif00/clinical_assistant.git
cd clinical_assistant

# 2. Create .env file with your HuggingFace token (required for Llama)
echo "HF_TOKEN=your_huggingface_token_here" > .env

# 3. Launch the application
docker-compose up --build

# 4. Access the application
# Open your browser at: http://localhost:8000
```

**That's it!** The application will:
- âœ… Install all dependencies automatically
- âœ… Load models on first request (takes ~30-60s)
- âœ… Serve both frontend and API from port 8000
- âœ… Cache models for fast subsequent requests

**Stopping the application:**
```bash
docker-compose down
```

---

### ğŸ’» Local Development

**For development or if you prefer running without Docker.**

#### Prerequisites

- Python 3.11+
- pip and virtualenv
- HuggingFace account and token
- 8GB+ RAM (16GB recommended)
- GPU optional but recommended

#### Step-by-Step Setup

```bash
# 1. Clone and navigate
git clone https://github.com/gortif00/clinical_assistant.git
cd clinical_assistant

# 2. Create virtual environment
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# 3. Install dependencies
cd backend
pip install -r requirements.txt

# 4. Set HuggingFace token
export HF_TOKEN="your_huggingface_token_here"

# 5. Run the application
uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload

# 6. Access the application
# Open your browser at: http://localhost:8000
```

**Development Tips:**
- Use `--reload` flag for auto-restart on code changes
- Check logs for model loading progress
- First request takes 30-60s (loads all models)
- Subsequent requests are 5-10s (uses cached models)

---

## ğŸ“ Project Structure

```
clinical_assistant/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/v1/
â”‚   â”‚   â”‚   â””â”€â”€ analyze.py          # API endpoints
â”‚   â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”‚   â””â”€â”€ config.py           # Configuration
â”‚   â”‚   â”œâ”€â”€ ml/
â”‚   â”‚   â”‚   â”œâ”€â”€ models_loader.py    # Model Manager (optimized)
â”‚   â”‚   â”‚   â””â”€â”€ pipeline.py         # Legacy pipeline (deprecated)
â”‚   â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”‚   â””â”€â”€ text_cleaning.py    # Text preprocessing
â”‚   â”‚   â”œâ”€â”€ main.py                 # FastAPI app entry point
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ models/                     # Trained models (not in repo)
â”‚   â”‚   â”œâ”€â”€ classifier/
â”‚   â”‚   â”œâ”€â”€ t5_summarizer/
â”‚   â”‚   â””â”€â”€ llama_peft/
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ requirements-dev.txt
â”œâ”€â”€ frontend/                       # Frontend files (served by FastAPI)
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ css/styles.css
â”‚   â””â”€â”€ js/app.js
â”œâ”€â”€ docs/                           # Documentation
â”‚   â”œâ”€â”€ REPORT_SUMMARY.md
â”‚   â”œâ”€â”€ SYSTEM_VERIFICATION_REPORT.md
â”‚   â””â”€â”€ SPEED_OPTIMIZATIONS.md
â”œâ”€â”€ docker-compose.yml              # Docker orchestration
â”œâ”€â”€ Dockerfile                      # Docker image definition
â”œâ”€â”€ start.sh                        # Quick start script
â”œâ”€â”€ .env                            # Environment variables (create this)
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md                       # This file
```

**Key Files:**
- `backend/app/main.py`: FastAPI app that serves both frontend and API
- `backend/app/ml/models_loader.py`: Optimized model manager with singleton pattern
- `frontend/`: Static frontend files (HTML/CSS/JS)
- `docker-compose.yml`: Single-command deployment configuration

---

## ğŸ“š API Documentation

Once the application is running, visit:

- **Interactive API Docs**: http://localhost:8000/docs
- **Alternative Docs**: http://localhost:8000/redoc

### Main Endpoints

#### `POST /api/v1/analyze`

Analyze a clinical case and generate recommendations.

**Request Body:**
```json
{
  "text": "Patient clinical observations...",
  "auto_classify": true,
  "pathology": null
}
```

**Response:**
```json
{
  "classification": {
    "pathology": "Major Depressive Disorder",
    "confidence": 0.87,
    "all_probabilities": {...}
  },
  "summary": "Clinical summary...",
  "recommendation": "Treatment recommendation...",
  "metadata": {
    "processing_time": 7.32
  }
}
```

#### `GET /api/v1/get_status`

Get execution device status (CUDA/MPS/CPU).

**Response:**
```json
{
  "status": "ok",
  "device": "mps"
}
```

#### `GET /api/v1/health`

Check if models are loaded and ready.

**Response:**
```json
{
  "status": "healthy",
  "models_loaded": true
}
```

---

## âš™ï¸ Configuration

### Environment Variables

Create a `.env` file in the project root:

```bash
# Required: HuggingFace token for Llama model access
HF_TOKEN=your_token_here

# Optional: Advanced settings (defaults work fine)
# CLASSIFICATION_MODEL_PATH=backend/models/classifier
# T5_SUMMARIZATION_PATH=backend/models/t5_summarizer
# LLAMA_MODEL_CHECKPOINT=meta-llama/Llama-3.2-1B-Instruct
```

### Getting a HuggingFace Token

1. Go to https://huggingface.co/settings/tokens
2. Create a new token (read access is sufficient)
3. Accept Llama model license at https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct
4. Add token to `.env` file

---

## ğŸ”§ Troubleshooting

### Common Issues

#### "Models not loaded" error
**Cause**: Models directory is empty or models failed to load.

**Solution**:
```bash
# Check if models exist
ls -lh backend/models/

# If missing, you need to train models first
# See docs/SYSTEM_VERIFICATION_REPORT.md for training instructions
```

#### First request is very slow (30-60s)
**Cause**: This is normal! Models are loading for the first time.

**Expected behavior**:
- First request: 30-60s (loads BERT + T5 + Llama)
- Second request: 5-10s (uses cached models)

#### "Frontend not showing device" or CORS errors
**Cause**: Backend might not be running or accessible.

**Solution**:
```bash
# Check backend is running
curl http://localhost:8000/api/v1/health

# Check logs for errors
docker-compose logs -f  # For Docker
# or check terminal output for local deployment
```

#### Docker build fails with "disk space" error
**Cause**: Docker models are large (~2GB).

**Solution**:
```bash
# Clean old images
docker system prune -a

# Use volume mounting (already configured in docker-compose.yml)
# Models will be loaded from ./backend/models instead of copied into image
```

### Performance Issues

If inference is slow even after first request:

1. **Check device**: Visit http://localhost:8000 - should show GPU if available
2. **Check logs**: Look for "âš¡ Already loaded, skip" messages (should appear on 2nd+ requests)
3. **Verify RAM**: Ensure you have 8GB+ free RAM
4. **GPU drivers**: For CUDA, ensure nvidia-docker is installed

---

## ğŸ“– Documentation

Additional documentation is available in the `docs/` directory:

- **[REPORT_SUMMARY.md](docs/REPORT_SUMMARY.md)**: Project overview and model details
- **[SYSTEM_VERIFICATION_REPORT.md](docs/SYSTEM_VERIFICATION_REPORT.md)**: Technical deep dive
- **[SPEED_OPTIMIZATIONS.md](docs/SPEED_OPTIMIZATIONS.md)**: Performance improvements explained

---

## âš ï¸ Important Disclaimer

**This system is a clinical decision support tool designed for licensed mental health professionals.**

- âœ… **Intended Use**: Augment professional clinical judgment
- âŒ **Not a Replacement**: Does not replace comprehensive clinical assessment
- ğŸ”’ **Professional Only**: For use by qualified healthcare providers
- ğŸ“‹ **Final Diagnosis**: Must incorporate patient history and professional expertise

**This tool should never be used as the sole basis for diagnosis or treatment decisions.**

---

## ğŸ“„ License

This project is licensed under the MIT License. See [LICENSE](LICENSE) for details.

---

## ğŸ™ Acknowledgments

- **Models**: BERT, T5, and Llama from HuggingFace Transformers
- **Framework**: FastAPI for unified backend/frontend serving
- **Optimization**: Inspired by efficient model management patterns

---

## ğŸ“§ Contact

For questions or issues:
- Open an issue on [GitHub](https://github.com/gortif00/clinical_assistant/issues)
- Check existing documentation in `docs/`

---

**Made with â¤ï¸ for Mental Health Professionals**
