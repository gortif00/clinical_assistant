# backend/app/api/v1/analyze.py
# API ENDPOINTS FOR CLINICAL CASE ANALYSIS
# This module defines the main API endpoints for analyzing clinical cases

from fastapi import APIRouter, HTTPException
from pydantic import BaseModel, Field
from typing import Optional

# Import the global model manager and device detection function
from app.ml.models_loader import manager, get_device
from app.core.config import MIN_TEXT_LENGTH

# Create API router with "analysis" tag for documentation grouping
router = APIRouter(tags=["analysis"])


# ============================================================================
# REQUEST/RESPONSE MODELS
# ============================================================================

class CaseRequest(BaseModel):
    """
    Input model for clinical case analysis requests.
    
    Attributes:
        text: The patient's clinical observations/case description
        auto_classify: If True, automatically detects condition; if False, uses manual pathology
        pathology: Manual condition selection (only used when auto_classify=False)
    """
    text: str = Field(..., min_length=MIN_TEXT_LENGTH, description="Patient clinical observations")
    auto_classify: bool = Field(default=True, description="Enable automatic classification")
    pathology: Optional[str] = Field(default=None, description="Manually selected pathology (if auto_classify is False)")


class CaseResponse(BaseModel):
    """
    Output model for analysis results.
    
    Attributes:
        classification: Dict with pathology, confidence, and all probabilities
        summary: Clinical summary of the case (generated by T5)
        recommendation: Treatment recommendations (generated by Llama)
        metadata: Processing information (text lengths, processing time, etc.)
    """
    classification: dict
    summary: str
    recommendation: str
    metadata: dict


# ============================================================================
# API ENDPOINTS
# ============================================================================


@router.get("/health")
def health_check():
    """
    Basic health check endpoint.
    
    Returns:
        Status indicating if critical models (classifier + summarizer) are loaded.
        This is used by Kubernetes/monitoring systems to verify the service is ready.
    """
    models_ready = manager.check_models_loaded()
    return {
        "status": "healthy" if models_ready else "degraded",
        "models_loaded": models_ready
    }


@router.get("/get_status")
def get_status():
    """
    Get the execution device status (CUDA/MPS/CPU).
    
    This endpoint is called by the frontend to display which hardware
    is being used for model inference (GPU vs CPU).
    
    Returns:
        Device type: 'cuda' (NVIDIA GPU), 'mps' (Apple Silicon GPU), or 'cpu'
    """
    device = get_device()
    return {
        "status": "ok",
        "device": device
    }


@router.post("/analyze", response_model=CaseResponse)
def analyze_case(data: CaseRequest):
    """
    Main endpoint: Analyze a clinical case and generate treatment recommendations.
    
    This endpoint runs a 3-stage pipeline:
    1. Classification: Detect mental health condition (BERT)
    2. Summarization: Generate clinical summary (T5)
    3. Generation: Create treatment recommendations (Llama)
    
    Supports two modes:
    - Auto-classify (default): Automatically detects the mental health condition
    - Manual: Uses a user-specified pathology for focused analysis
    
    Args:
        data: CaseRequest with clinical text and classification mode
        
    Returns:
        CaseResponse with classification, summary, recommendation, and metadata
        
    Raises:
        HTTPException(400): If text is too short (< MIN_TEXT_LENGTH)
        HTTPException(503): If models are not loaded
        HTTPException(500): If processing fails
    """
    
    # Validate input text length
    if len(data.text.strip()) < MIN_TEXT_LENGTH:
        raise HTTPException(
            status_code=400,
            detail=f"Text too short. Minimum {MIN_TEXT_LENGTH} characters required."
        )
    
    # Verify that critical models (classifier + summarizer) are loaded
    if not manager.check_models_loaded():
        raise HTTPException(
            status_code=503,
            detail="Models not loaded. Please ensure all models are available."
        )
    
    try:
        # Execute the optimized 3-stage pipeline
        # This is a consolidated function that runs all stages efficiently
        result = manager.process_request(
            text=data.text,
            auto_classify=data.auto_classify,
            pathology=data.pathology
        )
        
        # Check if the pipeline returned an error
        if "error" in result:
            raise HTTPException(status_code=500, detail=result["error"])
        
        return result
    
    except HTTPException:
        # Re-raise HTTP exceptions without modification
        raise
    except Exception as e:
        # Catch any unexpected errors and return as 500
        raise HTTPException(
            status_code=500,
            detail=f"Error during analysis: {str(e)}"
        )